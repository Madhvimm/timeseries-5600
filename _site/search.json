[
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "Time Series Data Visualization",
    "section": "",
    "text": "Importing Libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(imputeTS)\nlibrary(gridExtra)\nlibrary(reticulate)\nlibrary(readxl)\nuse_python(\"/Users/madhvimalhotra/anaconda3/envs/myenv/bin/python3\", require = T)\nknitr::knit_engines$set(python = reticulate::eng_python)\npy_install(\"tensorflow\")\n\n\n\nTime Series Data Visualization\nIn time series data visualization, the importance lies in presenting temporal patterns and trends in a clear and comprehensible manner. Effective visualization allows analysts and decision-makers to extract meaningful insights from the data, aiding in better understanding the dynamics of a system over time. The choice of visualization techniques is crucial, as it directly influences the interpretation of patterns within the time series.\nThe ability to discern seasonality, identify anomalies, and recognize patterns is vital for making informed predictions and strategic decisions. Furthermore, interactive features in visualizations enable users to delve deeper into the data, offering a dynamic and exploratory experience.\nUltimately, the clarity and accuracy of time series data visualization contribute significantly to enhancing decision-making processes across various domains, such as finance, healthcare, environmental monitoring and many other areas.\n\nStock Price Trends: NFLX, MSFT, and INTCBitcoin Price Movement AnalysisEthereum Market Fluctuations (Candlestick)Analyzing Unemployment Rate in USA over time (1948 - 2023)\n\n\nThis section provides an interactive line chart showing the stock price trends of Netflix, Microsoft, and Intel from 2012 to 2024. Users can hover over the chart to examine specific data points.\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"NFLX\", \"MSFT\", \"INTC\") \n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-10-01\",\n             to = \"2024-01-01\")}\n\nstock &lt;- data.frame(NFLX$NFLX.Adjusted, MSFT$MSFT.Adjusted, INTC$INTC.Adjusted)\nstock &lt;- data.frame(stock, rownames(stock))\ncolnames(stock) &lt;- append(tickers, 'Dates')\nstock$date &lt;- as.Date(stock$Dates, \"%Y-%m-%d\")\n\ng4 &lt;- \n  ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=NFLX, colour=\"NFLX\")) +\n  geom_line(aes(y=MSFT, colour=\"MSFT\")) +\n  geom_line(aes(y=INTC, colour=\"INTC\")) +\n  labs(\n    title = \"Stock Prices for Selected Companies\",\n    subtitle = \"From 2012-2024\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\"\n    ) +\n  theme_minimal() +\n  guides(colour=guide_legend(title=\"Companies\"))\n\n\nggplotly(g4) %&gt;%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\nThe visualization presents the adjusted closing stock prices for three companies — Netflix (NFLX), Microsoft (MSFT), and Intel (INTC) from 2012 to 2024. Over the last decade, Netflix shows a significant upward trend, reflecting substantial growth, particularly noticeable after 2016, with some volatility (esp post 2021) but a general upward momentum indicating investor confidence and potentially successful company performance. In contrast, Microsoft exhibits a steadier and consistent growth trajectory, suggesting ongoing positive developments and stable investor interest.\nIntel, however, displays relatively little change in stock price over the years, suggesting stable but stagnant growth in comparison to the other tech giants.\n\n\nAn analysis of Bitcoin’s price from 2020 to 2024 is presented here.\n\n\nCode\n#bitc_ALL &lt;- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2020-10-01\",src=\"yahoo\")\nbitc &lt;- getSymbols(\"BTC\",auto.assign = FALSE, from = \"2019-01-01\",src=\"yahoo\") \n#head(bitc)\n#start(bitc)\n#end(bitc)\n\nbitc=data.frame(bitc)\nbitc &lt;- data.frame(bitc,rownames(bitc))\n#head(bitc)\n\ncolnames(bitc)[7] = \"date\"\n#head(bitc)\n\nbitc$date&lt;-as.Date(bitc$date,\"%Y-%m-%d\")\n#str(bitc)\n\n\n## plotly\nbit_coin &lt;- plot_ly(bitc, x = ~date, y = ~BTC.Adjusted, type = 'scatter', mode = 'lines')\n\nbit_coin &lt;- bit_coin %&gt;% layout(title = \"Bitcoin Price Trend Analysis (2020-2024)\")\nbit_coin\n\n\n\n\n\n\nThe graph above shows Bitcoin’s price changes over four years. It starts with a steady price, then a big drop in the middle of 2022, which could mean people were selling off a lot of Bitcoin at that time. After the drop, the price goes up and down a lot but generally rises, suggesting that people became more interested in buying Bitcoin again. This is common in the Crypto-currency market, where prices can change a lot based on what people feel about the future of cryptocurrencies.\n\n\nThis section features a candlestick chart illustrating Ethereum’s market price fluctuations between 2020 and 2024.\n\n\nCode\n# Get Ethereum data using quantmod\neth_data &lt;- getSymbols(\"ETH-USD\", src = \"yahoo\", from = \"2020-01-01\", to = Sys.Date(), auto.assign = FALSE)\n\n# Convert the data to a data frame for plotly\neth_df &lt;- data.frame(Date = index(eth_data),\n                     Open = as.numeric(eth_data[,1]),\n                     High = as.numeric(eth_data[,2]),\n                     Low = as.numeric(eth_data[,3]),\n                     Close = as.numeric(eth_data[,4]))\n\n# Use plotly to create a candlestick chart\nfig &lt;- plot_ly(eth_df, x = ~Date, type = \"candlestick\",\n        open = ~Open, close = ~Close,\n        high = ~High, low = ~Low) %&gt;%\n  layout(title = \"Ethereum Candlestick Chart\")\n\n# Print the figure to display the plot\nfig\n\n\n\n\n\n\nThe “Ethereum Candlestick Chart” above shows how the value of Ethereum changed from 2020 to 2024. It started off quite low, then shot up quickly in 2021. After that, it fell sharply, which might mean that people were less interested or there were some big changes in the crypto market. The price went up and down for a while, this chart is useful because it gives a clear picture of the highs and lows of Ethereum’s price over time.\n\n\nThis section provides an interactive line chart showing the unemployment rate in the USA from the 1950s to the present.\n\n\nCode\n# Set the FRED API key, which you can obtain for free by signing up on the FRED website\nsetSymbolLookup(UNRATE = list(src=\"FRED\"))\n\n\n# Get Unemployment Rate data from FRED\nunrate_data &lt;- getSymbols(\"UNRATE\", src = \"FRED\", auto.assign = FALSE)\n\n# Convert the data to a data frame for plotly\nunrate_df &lt;- data.frame(Date = index(unrate_data),\n                        UnemploymentRate = as.numeric(unrate_data[,1]))\n\n# Use plotly to create an interactive line chart\nunrate_plot &lt;- plot_ly(unrate_df, x = ~Date, y = ~UnemploymentRate, type = 'scatter', mode = 'lines', name = 'Unemployment Rate') %&gt;%\n  layout(title = \"US Unemployment Rate Over Time\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Unemployment Rate (%)\"))\n\n# Print the figure to display the plot\nunrate_plot\n\n\n\n\n\n\nThe chart above shows the unemployment rate in the US from the 1950s. It goes up and down, showing times when jobs were hard to find or more plentiful. There’s a really big jump in unemployment in 2020, which was probably because of the pandemic crisis when a lot of people lost their jobs all at once. But then it gets better pretty fast."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Deciphering Trends in Water Quality with Time Series Analysis\nIn this project, we’re looking at how water quality has changed over time, focusing on harmful substances such as lead, arsenic, and nitrate. We’re using data from the Environmental Protection Agency’s (EPA) Six-Year Reviews to see how the amounts of these contaminants in drinking water have shifted. Understanding these changes is crucial for ensuring our water is safe and healthy.\nThe EPA checks the rules for drinking water quality every six years to keep up with new scientific findings and technology. This process is important for making sure our drinking water stays safe. By examining this data, we want to see how effective current rules are at keeping contaminants at low levels and where we might need to improve.\nOur main goal is to look at the trends in how much lead, arsenic, and nitrate are in our water. This analysis helps us understand if our water is getting cleaner or if more work needs to be done to reduce these harmful substances. This project is about more than just numbers; it’s about making sure our communities have access to clean and safe drinking water."
  },
  {
    "objectID": "financial-ts-models.html",
    "href": "financial-ts-models.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Financial Time Series Models (ARCH/GARCH)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular timestamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "deep-learning-ts.html",
    "href": "deep-learning-ts.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Deep Learning for TS"
  },
  {
    "objectID": "univariate-ts-models.html",
    "href": "univariate-ts-models.html",
    "title": "Univariate TS Models (ARIMA/SARIMA)",
    "section": "",
    "text": "Univariate TS Models (ARIMA/SARIMA)"
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Data Sources\nIn our journey to uncover trends in water quality, we’re leveraging critical datasets from the Environmental Protection Agency’s (EPA) Six-Year Reviews. This comprehensive dataset examines the safety and quality of drinking water across three key periods—SYR2, SYR3, and SYR4—showcasing the EPA’s dedication to maintaining high drinking water standards through periodic reassessment based on the latest scientific research and technological advancements. A team of experts has rigorously quality-checked this data, ensuring its accuracy and reliability for in-depth analysis.\nPrimary Data Sources:\n\nEIDC Dataset: This broad dataset provides an overarching view of water quality data, serving as our analytical starting point. It encompasses a wide array of contaminants and offers insights into national water quality trends.\n\nhttps://redivis.com/EIDC/admin/datasets/cxcg-ahxafrqhj\n\nMicrobial and Disinfection Byproduct Data (2012-2019): Critical for understanding the outcomes of the Fourth Six-Year Review, this dataset provides insights into contaminants under the microbial and disinfection byproduct rules.\n\nhttps://www.epa.gov/dwsixyearreview/microbial-and-disinfection-byproduct-data-files-2012-2019-epas-fourth-six-year\n\nSix-Year Review 3 Compliance Monitoring Data (2006-2011): This dataset includes compliance monitoring data crucial for the third review, allowing for a detailed examination of water quality standards.\n\nhttps://www.epa.gov/dwsixyearreview/six-year-review-3-compliance-monitoring-data-2006-2011\n\nSix-Year Review 2 Contaminant Occurrence Data (1998-2005): Offering a historical perspective, this dataset from SYR2 provides a foundation for analyzing trends over time.\n\nhttps://www.epa.gov/dwsixyearreview/six-year-review-2-contaminant-occurrence-data-1998-2005\nEach dataset not only informs our understanding of historical and current water quality but also plays a crucial role in identifying areas for regulatory improvement and technological intervention. By integrating my academic knowledge with professional experience in policy implementation and sustainable development, this project aims to distill complex information into actionable insights, fostering public good through enhanced water quality management and policy development."
  },
  {
    "objectID": "multivariate-ts-models.html",
    "href": "multivariate-ts-models.html",
    "title": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)",
    "section": "",
    "text": "Multivariate TS Models (ARIMAX/SARIMAX/VAR)"
  },
  {
    "objectID": "exploratory-data-analysis.html",
    "href": "exploratory-data-analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About You",
    "section": "",
    "text": "Welcome to my portfolio! I’m a dedicated professional at the nexus of policy implementation, data analytics, and sustainable development, currently pursuing MS in Data Science and Public Policy at Georgetown University. My journey has taken me from the diverse landscapes of rural India to the analytical rigor of academia in the United States, driving forward the agenda of data-driven governance and policy effectiveness.\nWith a diverse, I’ve harnessed the power of data to illuminate the path for decision-makers in government, think tanks, and NGOs. My toolkit, fortified by advanced analytics techniques and an array of industry-standard tools including Python, R, and Tableau, has been instrumental in crafting informed strategies that propel sustainable growth and impactful policy implementation.\nJoin me as I explore the intersections of data analytics and policy implementation, where each insight serves as a stepping stone towards more equitable and efficient governance."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Conclusions"
  },
  {
    "objectID": "other-ts-methods.html",
    "href": "other-ts-methods.html",
    "title": "Other: Interrupted TS/ARFIMA/ Spectral Analysis",
    "section": "",
    "text": "Other: Interrupted TS/ARFIMA/ Spectral Analysis"
  }
]